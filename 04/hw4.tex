\documentclass[a4paper,11pt]{article}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{setspace}
\usepackage{parskip}
\onehalfspacing
\begin{document}

\author{Hiromasa Okada}
\title{\vspace{-2cm}Report for Sheet 4\\
\small{Lab Course Machine Learning and Data Analysis}}
\maketitle

\section*{Implementation comments}
In this exercise I implemented support vector machine and plot function of it. There are two type of support vector machines, the first one is trained by SMO algorithum and the second one is with direct solving of quadratic programming.

\begin{verbatim}
# SVM class with SMO algorithum
smo_svm = svm_smo(kernel, C, kernelparameter) 

# SVM class with quadratic problem
qb_svm=svm_qp(kernel, C, kernelparameter) 

\end{verbatim}

Both class get $kernel \in \{ 'gaussian','polynomial','linear' \}$ and C as a contraint parameter for lagrange multipliers. Both two classes has same kernel function "getkernel(self, X, Y=None)" and predict function "fx(self,X1,X2,Y)". 
On the other hands SMO class has following functions which QP class does not have.

\begin{verbatim}
_compute_box_constraints(self, i, j, Y, alpha, C)
_update_parameters(self, E_i, E_j, i, j, K, Y, alpha, b, C)
_compute_updated_b(self, E_i, E_j, i, j, K, Y, alpha_old, alpha_new, b_old, C)
\end{verbatim}

With these functions SMO class finds iteratively optimal lagrange multipliers and bias. But QP class solve quadratic programming in the function "fit(self, X, Y)" without functions above.
In both classes lagrange multipliers, bias, support vector and labels are saved as object like

\begin{verbatim}
self.SV= X[alp_idx]
self.y = Y[alp_idx]
self.alpha = self.alpha[alp_idx]
self.b = np.mean(self.y - self.fx(self.SV,self.SV,self.y)[:,0])
\end{verbatim}



\section*{Assignment 3}
The formular of the SVM optimization problem and the quadratic programming are given in 8th page of the slide.


\begin{figure}[htbp]
  \includegraphics[scale=0.4]{qpform.png}
\end{figure}

The relation between them: 

\begin{eqnarray*}
\max_{\alpha} \:\:\:\:\:\:   \sum^N_{i=1} \alpha_i -\frac{1}{2} \sum^N_{i,j=1} \alpha_i \alpha_j y_i y_j k(x_i,x_j) \\
= \min_{\alpha} \:\:\:\:\:\:  -\sum^N_{i=1} \alpha_i +\frac{1}{2} \sum^N_{i,j=1} \alpha_i \alpha_j y_i y_j k(x_i,x_j)
\end{eqnarray*}
\begin{eqnarray*}
x= (\alpha_1,...,\alpha_N)^T, \:\: q=(-1,...-1)^T  
\end{eqnarray*}
\begin{eqnarray*}
P = Y \odot K, \:\:\:\:\:\: \mbox{Where} \odot \mbox{elementwise multiplication and }Y, K \in \mathbf{R}^{N \times N} \\
\end{eqnarray*}
\begin{eqnarray*}
Y = yy^T, \:\:\: y=(y_1,...y_i)^T \in \{ -1, 1\}^N, \:\:\:
K = 
\left(
\begin{array}{cccc}
k(x_1,x_1) & \cdots & k(x_1,x_j) \\
 \vdots & \ddots &  \vdots  \\
 k(x_i,x_1) & \cdots & k(x_i,x_j)
\end{array}
\right)
\end{eqnarray*}
\begin{eqnarray*}
G = 
\left[
\begin{array}{cccc}
-I \\
 I 
\end{array}
\right],  \:\:\:
h = 
\left(
\begin{array}{cccc}
0 \\
 \vdots  \\
0  \\
C \\
 \vdots  \\
C  \\
\end{array}
\right)
\:\:\:\: \mbox{Where } I \mbox{ is identical matrix and } G \in \mathbf{R}^{2N \times N}, \:\:\: h \in \mathbf{R}^{2N}
\end{eqnarray*}
\begin{eqnarray*}
A = (y_1,...,y_N) \in \mathbf{Z}^{1 \times N}, \:\: \forall_{i=0}^N \: y_i \in \{-1,1\}, \:\:\: b = [0] \in \mathbf{R}^{1 \times 1}
\end{eqnarray*}


\section*{Assignment 4}

\subsection*{1. Find parameters C and σ for a Gaussian kernel}
To find parameters C and σ for a Gaussian kernel I used the cross validation with following parameters

\begin{verbatim}
para = { 'kernel': ['gaussian'], 'kernelparameter': np.logspace(-2,2,10), 
		'regularization': np.linspace(1.0,3.0,10)}
\end{verbatim}

Then the parameters C and σ for a Gaussian kernel are

\begin{verbatim}
Kernelparameter:  0.599484250319
C:  2.77777777778
\end{verbatim}

After the plot of predictons of training data and test data I found that it classfied well and there was no over fitting and no under fitting by traing data so that it classfied also test data well.

\begin{verbatim}



\end{verbatim}


\begin{figure}[htbp]
  \includegraphics[scale=0.5]{trcvsmo.png}
  \includegraphics[scale=0.5]{tecvsmo.png}
\end{figure}

\subsection*{2. Over fitting and under fitting}
Which parameter C and σ for a Gaussian kernel overfit and underfit?
The parameters which cause underfitting are

\begin{verbatim}
Kernelparameter:  100.0
C:  0.7
\end{verbatim}

and plots are

\begin{figure}[htbp]
  \includegraphics[scale=0.5]{smouftr.png}
  \includegraphics[scale=0.5]{smoufte.png}
\end{figure}

In case of the under fitting the classfier will be a linear.

\begin{verbatim}

\end{verbatim}

The parameters which cause overfitting are

\begin{verbatim}
Kernelparameter:  0.1
C:  3.7
\end{verbatim}

and plots are

\begin{figure}[htbp]
  \includegraphics[scale=0.5]{smooftr.png}
  \includegraphics[scale=0.5]{smoofte.png}
\end{figure}

In case of the overfitting there are more than two clusters even we don't need more.
When the sigma is higher and C is lower than optimal it tends to underfit, on the other hands the sigma is lower and C is higher than optimal it tends to overfit.

\subsection*{3. ROC curve with different bias}

For optimal C and $\sigma$, a receiver operator characteristics (ROC) curve is plotted by varying the bias
parameter b of your SVM model. 

\begin{figure}[htbp]
  \includegraphics[scale=0.5]{roc.png}
\end{figure}

The used possible bais are in this range:

\begin{verbatim}
bias = np.linspace(-10,10,110)
\end{verbatim}


\section*{Assignment 5}

\begin{verbatim}


























\end{verbatim}

\section*{Assignment 6}
First to visualize the separation by a hyperplane the dimensions of datas are reduced by PCA.

\begin{verbatim}
pca= PCA(X)
X_pro = pca.project(X,2)
\end{verbatim}

In case of linear separation class 1 and class 3 are well separated from other classes but class 2 are not. Because class 2 lies in the center of classes and it is not possible to separate by a linear line.

\begin{figure}[htbp]
  \includegraphics[scale=0.3]{l1_23.png}
  \includegraphics[scale=0.3]{l2_13.png}
  \includegraphics[scale=0.3]{l3_12.png}
\end{figure}

The losses for three linear separations are

\begin{verbatim}
Loss of linearly separation between class 1 and others:  0.0
Loss of linearly separation between class 2 and others:  0.2814814814814815
Loss of linearly separation between class 3 and others:  0.037037037037037035
\end{verbatim}

In case of non-linear separation(gaussian) all of three classes are separated well. Non-linear separation line can be a circle line therefore it is also usuful for classification of this dataset.

\begin{figure}[htbp]
  \includegraphics[scale=0.3]{nl1_23.png}
  \includegraphics[scale=0.3]{nl2_13.png}
  \includegraphics[scale=0.3]{nl3_12.png}
\end{figure}

\begin{verbatim}
Loss of non-linearly separation between class 1 and others:  0.0
Loss of non-linearly separation between class 2 and others:  0.2814814814814815
Loss of non-linearly separation between class 3 and others:  0.037037037037037035
\end{verbatim}

\section*{Assignment 7}
\subsection*{1. The best kernel and kernel parameters for each digit}

After the Cross Validation with linear kernel and c in range "np.linspace(0.1,3.0,10)" we got following results.

\begin{verbatim}
Best C for the digit  0  by the Linear Kernel C= 0.422222222222
The Error rate:  0.00498256103637
Best C for the digit  1  by the Linear Kernel C= 0.1
The Error rate:  0.00348779272546
Best C for the digit  2  by the Linear Kernel C= 0.1
The Error rate:  0.0189337319382
Best C for the digit  3  by the Linear Kernel C= 0.422222222222
The Error rate:  0.00896860986547
Best C for the digit  4  by the Linear Kernel C= 0.422222222222
The Error rate:  0.00996512207275
Best C for the digit  5  by the Linear Kernel C= 0.744444444444
The Error rate:  0.00697558545092
Best C for the digit  6  by the Linear Kernel C= 1.71111111111
The Error rate:  0.0
Best C for the digit  7  by the Linear Kernel C= 3.0
The Error rate:  0.000498256103637
Best C for the digit  8  by the Linear Kernel C= 2.67777777778
The Error rate:  0.0104633781764
Best C for the digit  9  by the Linear Kernel C= 0.422222222222
The Error rate:  0.01096163428
\end{verbatim}


The results with polynormal kernel, c in range "np.linspace(1.0,3.0,5) and kernelparameter in range "np.logspace(1,5,5)"

\begin{verbatim}
Best C for the digit  0  by the Polynomal Kernel C= 1.0
And degree p=  10.0
The Error rate:  0.178873941206
Best C for the digit  1  by the Polynomal Kernel C= 1.0
And degree p=  10.0
The Error rate:  0.00946686596911
Best C for the digit  2  by the Polynomal Kernel C= 1.0
And degree p=  10.0
The Error rate:  0.0986547085202
Best C for the digit  3  by the Polynomal Kernel C= 1.0
And degree p=  10.0
The Error rate:  0.0827105132038
Best C for the digit  4  by the Polynomal Kernel C= 1.0
And degree p=  10.0
The Error rate:  0.0996512207275
Best C for the digit  5  by the Polynomal Kernel C= 1.0
And degree p=  10.0
The Error rate:  0.079720976582
Best C for the digit  6  by the Polynomal Kernel C= 1.0
And degree p=  10.0
The Error rate:  0.0847035376183
Best C for the digit  7  by the Polynomal Kernel C= 2.0
And degree p=  10.0
The Error rate:  0.0568011958146
Best C for the digit  8  by the Polynomal Kernel C= 1.0
And degree p=  10.0
The Error rate:  0.0827105132038
Best C for the digit  9  by the Polynomal Kernel C= 1.0
And degree p=  10.0
The Error rate:  0.0881913303438
\end{verbatim}

The results with gaussian kernel, c in range "np.linspace(1.0,3.0,5) and kernelparameter in range "np.linspace(1/1.0,1/5.0,10)"


\begin{verbatim}
Best C for the digit  0  by the Gaussian Kernel C= 0.825
And kernel width w=  1.31558702896
The Error rate:  0.0
Best C for the digit  1  by the Gaussian Kernel C= 1.55
And kernel width w=  0.948683298051
The Error rate:  0.000498256103637
Best C for the digit  2  by the Gaussian Kernel C= 0.1
And kernel width w=  0.707106781187
The Error rate:  0.0986547085202
Best C for the digit  3  by the Gaussian Kernel C= 0.1
And kernel width w=  0.707106781187
The Error rate:  0.0827105132038
Best C for the digit  4  by the Gaussian Kernel C= 0.1
And kernel width w=  0.707106781187
The Error rate:  0.0996512207275
Best C for the digit  5  by the Gaussian Kernel C= 0.1
And kernel width w=  0.707106781187
The Error rate:  0.079720976582
Best C for the digit  6  by the Gaussian Kernel C= 0.825
And kernel width w=  1.58113883008
The Error rate:  0.0279023418037
Best C for the digit  7  by the Gaussian Kernel C= 0.825
And kernel width w=  1.58113883008
The Error rate:  0.0189337319382
Best C for the digit  8  by the Gaussian Kernel C= 0.1
And kernel width w=  0.707106781187
The Error rate:  0.0827105132038
Best C for the digit  9  by the Gaussian Kernel C= 2.275
And kernel width w=  1.31558702896
The Error rate:  0.000498256103637

\end{verbatim}


\end{document}

