\documentclass[a4paper,11pt]{article}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{setspace}
\usepackage{parskip}
\onehalfspacing
\begin{document}

\author{Hiromasa Okada}
\title{\vspace{-2cm}Report for Sheet 3\\
\small{Lab Course Machine Learning and Data Analysis}}
\maketitle

\section*{Implementation comments}
In this exercise I implemented Cross-validation function 

\begin{verbatim}
def cv(X, y, method, parameters, nfolds, nrepetitions, loss function)
\end{verbatim}
and Kernel Ridge Regression class 

\begin{verbatim}
class krr(kernel, kernelparameter, regularization)
\end{verbatim}

The cross validation function will find the best parameter combination for Kernel Ridge Regression by computing loss of the prediction and find the parameters which make the loss to be minimum. 
Kernel Ridge Regression class saves kernel, kernelparameter, regularization as object when this class is constructed. This class has following three functions 

\begin{verbatim}
def getkernel(self, X, Y=None):
def fit(self, X, y):
def predict(self, X):
\end{verbatim}
 
X is dataset and y is true values. The getkernel() will get a kernel. It can make a kernel from one dataset and from two dataset. The default is with one dataset. When fit() is called, dataset X is saved as an object Xfit and the coefficients $\alpha$ is saved as an object alpha. The function predict will estimate Y. This function first gets a kernel from a new kernel from Xfit and new dataset X then estimate Y by using the saved alpha.

\begin{verbatim}


\end{verbatim}


\section*{Assignment 3}
We know the following distributions

\begin{eqnarray*}
p(x|y = -1) \sim N(\mu = 0; \sigma = 1) \\
p(x|y = +1) \sim N(\mu = 2; \sigma = 1)
\end{eqnarray*}

Then we will plot the graph of this by following way.

\begin{verbatim}
def nomdis(x,mu, sigma):
    return np.exp(-((x-mu)**2)/(2*sigma**2))/np.sqrt(2*np.pi*sigma**2)
    
pp1 = nomdis(np.arange(-4,6,0.1),0,1)
pp2 = nomdis(np.arange(-4,6,0.1),2,1)
plt.plot(np.arange(-4,6,0.1),nomdis(np.arange(-4,6,0.1),0,1),label='mu=0,std=1')
plt.plot(np.arange(-4,6,0.1),nomdis(np.arange(-4,6,0.1),2,1),label='mu=2,std=1')
plt.legend()
plt.title("Distributions")
\end{verbatim}

\begin{figure}[htbp]
  \includegraphics[scale=0.7]{histmix.png}
\end{figure}

And we can calculate the ROC curve analytically by using the probability density function of gaussian. I first had to decide the enough long interval of random variables. In case of the upper distribution I chose np.arange(-4,6,0.1). From -4 to 6 the threshold is slided with step interval 0.1. At each step the right side of the area of 1st distribution and 2nd distribution are calicurated. The right side of the area of 1st distribution divided by whole area of 1st distribution is false positive rate and The right side of the area of 2nd distribution divided by whole area of 2nd distribution is true positive rate.

\begin{verbatim}
s1 = pp1.sum()
s2 = pp2.sum()
c1 = np.cumsum(pp1)/np.cumsum(pp1)[-1]
c2 = np.cumsum(pp2)/np.cumsum(pp2)[-1]
fp = 1-c1
tp =1-c2
plt.plot(fp,tp)
plt.title("Analytical ROC")
\end{verbatim}

\begin{figure}[htbp]
  \includegraphics[scale=0.7]{roc.png}
\end{figure}

After that the empirical ROC curve is calicurated with n datasets by the following algorithum. And ploted with analytical ROC curve.

\begin{verbatim}
def ROCcul(P):
    f= np.linspace(-4,6,21)#np.arange(11)-4
    ptrue = np.array(P[:,1])
    ttp = ptrue.sum()
    tfp = len(ptrue)-ttp
    roc = np.zeros((2,len(f)))
    for i in range(len(f)):
        pred =np.zeros(len(P))
        pred[np.where(P[:,0]>f[i])]=1
        roc[1][i] = ptrue[np.where(pred==1)].sum()/ttp
        roc[0][i] = pred[np.where(ptrue==0)].sum()/tfp
    ROC=roc[:,np.argsort(roc[0])]
    return ROC

N = np.arange(1000,9000,1000)
plt.figure(figsize=(7,7))
plt.plot(fp,tp,label="Analytical ROC",linewidth=3.0)
cl = ['black','red','blue','yellow','black','red','blue','yellow']
line = ['solid','solid','solid','solid','dashed','dashed','dashed','dashed']
ite = 0
for n in N:
    p1 = np.random.randn(n/2)
    p2 = np.random.randn(n/2) +2
    P1 = np.append(p1.reshape(1,len(p1)),np.zeros((1,len(p1))),0)
    P2 = np.append(p2.reshape(1,len(p2)),np.ones((1,len(p2))),0)
    P = np.append(P1,P2,1)
    P =P.T[np.argsort(P[0])]
    ptrue = np.array(P[:,1])
    sroc = ROCcul(P)
    plt.plot(sroc[0],sroc[1], label=("Emprical n=",n),color=cl[ite],linestyle=line[ite])
    ite = ite+1
plt.legend(loc="lower right")
plt.xlabel("FP rate")
plt.ylabel("TP rate")
plt.title("ROC curve")
\end{verbatim}

\begin{verbatim}








\end{verbatim}

\begin{figure}[htbp]
  \includegraphics[scale=0.6]{rocall.png}
\end{figure}

The analytical ROC curve is much smoother than all empirical ROC curves. And it lies on middle of empirical ROC curves. In case of empirical ROC curve The more sample data points are, the clother the ROC curve seems to lie on the upper left side. But this difference is not much remarkable.

	


\section*{Assignment 4}
\subsection*{Results}


\begin{verbatim}
{'banana': {'cvloss': 2.9000000000000012,
  'kernel': 'gaussian',
  'kernelparameter': 1.6681005372000592,
  'regularization': 0.0001,
  'ypred': array([[ 1.04544672],
         [-1.0211602 ],
         [-1.11588551],
         ..., 
         [-0.01545813],
         [-0.71081767],
         [-0.58393764]])},
 'diabetis': {'cvloss': 10.690909090909091,
  'kernel': 'gaussian',
  'kernelparameter': 35.938136638046259,
  'regularization': 0.0001,
  'ypred': array([[ -1.95767968e-01],
         [ -3.65473188e-01],
         [  6.73930210e-01],
         ..., 
         [ -1.25682744e+00],
         [ -5.49648731e-01],
         [ -3.31448477e-01]])},
 'flare-solar': {'cvloss': 11.663636363636362,
  'kernel': 'linear',
  'kernelparameter': 0,
  'regularization': 0.0001,
  'ypred': array([[-0.24471171],
         [-0.24471171],
         [-0.27743741],
         ..., 
         [-0.31016311],
         [-0.24471171],
         [-0.24471171]])},
 'image': {'cvloss': 5.7999999999999998,
  'kernel': 'linear',
  'kernelparameter': 0,
  'regularization': 0.0001,
  'ypred': array([[ 0.69327584],
         [-0.66020007],
         [ 0.26247227],
         ..., 
         [-0.53325973],
         [-0.05732539],
         [ 0.06164567]])},
 'ringnorm': {'cvloss': 0.0,
  'kernel': 'gaussian',
  'kernelparameter': 1.6681005372000592,
  'regularization': 0.0001,
  'ypred': array([[-0.30476664],
         [-1.03591385],
         [ 0.09470157],
         ..., 
         [-0.0018193 ],
         [-0.01799695],
         [ 0.00646591]])}}
\end{verbatim}



\subsection*{ROC and AUC}

To calicurate ROC, AUC and loss I defined a following function. This function returns only when plot = True. Therefore it works only as a loss function during the cross varidation but if we caricurate the loss of the test, we can plot a ROC curve and print AUC value.

\begin{verbatim}
def roc_fun(y_true, y_pred, plot = False):
    assert(len(y_true) == len(y_pred))
    bins=100
    pred = np.array(y_pred[:,0])
    true = np.array(y_true[:,0])
    true[np.where(true==-1)]=0
    n=len(pred)
    thres = np.linspace(np.min(pred),np.max(pred),bins).reshape(bins,1)*np.ones((1 ,n))
    predbr = pred.reshape(1,n)*np.ones((bins ,1))
    prebl=(predbr>thres).astype(np.int64)
    comp = (prebl==true).astype(np.int64)
    tpr = (prebl*comp).sum(1)/true.sum()
    fpr = (((comp-prebl))==(-np.ones(comp.shape))).sum(1)/(n-true.sum())
    idx=np.argsort(fpr)
    roc = np.append(fpr.reshape(1,len(fpr)),tpr.reshape(1,len(tpr)),0)
    D = np.linalg.norm(roc - np.array([[0],[1]]),axis=0)
    minidx=np.argmin(D)
    loss=(n-comp[minidx].sum())/n
    ROC = roc[:,idx]
    
    if(plot==True):
        fpdif = ROC[0,1:]-ROC[0,:-1]
        lower = fpdif*ROC[1,:-1]
        upper = fpdif*ROC[1,1:]
        AUC = (lower.sum()+upper.sum())/2
        print("AUC = ",AUC)
        plt.plot(ROC[0],ROC[1])
        plt.title("ROC curve")
        plt.xlabel("FP")
        plt.ylabel("TP")
    return loss 
\end{verbatim}

The plots of best results of ROC and AUC are following.

\begin{verbatim}
ROC curve of the classification of Banana 
AUC = 0.958047060608
\end{verbatim}

\begin{figure}[htbp]
 \centering 
  \includegraphics[scale=0.7]{rocbg.png}\\
    \caption{ROC curve of the Banana}
\end{figure}

\begin{verbatim}




\end{verbatim}

\begin{verbatim}
ROC curve of the classification of diabetis 
AUC = 0.856300697982
\end{verbatim}
\begin{figure}[htbp]
 \centering 
  \includegraphics[scale=0.6]{rocdl.png}\\
    \caption{ROC curve of the diabetis}
\end{figure}

\begin{verbatim}


\end{verbatim}

\begin{verbatim}
ROC curve of the classification of flare-solar 
AUC = 0.699492063492
\end{verbatim}

\begin{figure}[htbp]
 \centering 
  \includegraphics[scale=0.6]{rocfl.png}\\
  \caption{ROC curve of the flare-solar}
\end{figure}

\begin{verbatim}



\end{verbatim}

\begin{verbatim}
ROC curve of the classification of image 
AUC = 0.870657722094
\end{verbatim}

\begin{figure}[htbp]
 \centering 
  \includegraphics[scale=0.6]{rocil.png}\\
    \caption{ROC curve of the of image }
\end{figure}

\begin{verbatim}



\end{verbatim}


\begin{verbatim}
ROC curve of the classification of ringnorm
AUC = 0.995704888801
\end{verbatim}

\begin{figure}[htbp]
 \centering 
  \includegraphics[scale=0.6]{rocrl.png}\\
    \caption{ROC curve of the ringnorm }
\end{figure}

After the calculation I found that for the banana data and ringnorm data the classification with ridge regression works very well. But for the flare-solar's data AUC is much smaller than others.




\end{document}

